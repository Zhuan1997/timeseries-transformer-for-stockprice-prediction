{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1942cbad-6ea7-42b9-b3fa-2a49bb6ada79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TimeSeriesTransformerConfig\n",
    "from transformers import TimeSeriesTransformerForPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3b5aab-38e5-4cdb-916c-832cc4a9c0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesTransformerConfig {\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"cardinality\": [\n",
       "    0\n",
       "  ],\n",
       "  \"context_length\": 30,\n",
       "  \"d_model\": 64,\n",
       "  \"decoder_attention_heads\": 2,\n",
       "  \"decoder_ffn_dim\": 32,\n",
       "  \"decoder_layerdrop\": 0.1,\n",
       "  \"decoder_layers\": 2,\n",
       "  \"distribution_output\": \"student_t\",\n",
       "  \"dropout\": 0.1,\n",
       "  \"embedding_dimension\": [\n",
       "    0\n",
       "  ],\n",
       "  \"encoder_attention_heads\": 2,\n",
       "  \"encoder_ffn_dim\": 32,\n",
       "  \"encoder_layerdrop\": 0.1,\n",
       "  \"encoder_layers\": 2,\n",
       "  \"feature_size\": 27,\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 4,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"lags_sequence\": [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    4\n",
       "  ],\n",
       "  \"loss\": \"nll\",\n",
       "  \"model_type\": \"time_series_transformer\",\n",
       "  \"num_dynamic_real_features\": 0,\n",
       "  \"num_parallel_samples\": 100,\n",
       "  \"num_static_categorical_features\": 0,\n",
       "  \"num_static_real_features\": 0,\n",
       "  \"num_time_features\": 3,\n",
       "  \"prediction_length\": 10,\n",
       "  \"scaling\": \"mean\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"use_cache\": true\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a Time Series Transformer configuration with 10 time steps for prediction\n",
    "configuration = TimeSeriesTransformerConfig(prediction_length=10,\n",
    "                                           context_length=30,\n",
    "                                           distribution_output='student_t',\n",
    "                                           input_size=4,\n",
    "                                           loss = 'nll',\n",
    "                                           lags_sequence=[1,2,3,4],\n",
    "                                           num_time_features=3,\n",
    "                                           cardinality=None\n",
    "                                            )\n",
    "\n",
    "# Randomly initializing a model (with random weights) from the configuration\n",
    "model = TimeSeriesTransformerForPrediction(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e5aebf-d69d-4071-81ef-40a60d540d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([185281, 34, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a past_observed_mask tensor with shape [1000, 30,4], all elements equal to 1\n",
    "past_observed_mask_tensor = torch.ones((185281, 34,4), dtype=torch.float)\n",
    "\n",
    "# Check the shape and first few elements of the tensor\n",
    "past_observed_mask_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6774f4-c68c-4066-9bbd-6e8c66ffd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_time_features = torch.load('past_time_features_tensor.pt')\n",
    "future_time_features=torch.load('future_time_features_tensor.pt')\n",
    "past_values_tensors=torch.load('past_value_tensor.pt')\n",
    "future_values_tensors=torch.load('future_value_tensor.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5f5faf2-a9f8-4533-a010-1dad75a7e4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([185281, 34, 3]), torch.Size([185281, 34, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_time_features.shape, past_values_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5fd9d2-bcf3-4b9a-a17f-9614f787a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Split the dataset into training and validation sets (e.g., 80% training, 20% validation)\n",
    "train_past_values, val_past_values, \\\n",
    "train_past_time_features, val_past_time_features, \\\n",
    "train_past_observed_mask, val_past_observed_mask, \\\n",
    "train_future_values, val_future_values, \\\n",
    "train_future_time_features, val_future_time_features = train_test_split(\n",
    "    past_values_tensors, past_time_features, past_observed_mask_tensor, \n",
    "    future_values_tensors, future_time_features, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets for training and validation\n",
    "train_dataset = TensorDataset(train_past_values, train_past_time_features, train_past_observed_mask, train_future_values, train_future_time_features)\n",
    "val_dataset = TensorDataset(val_past_values, val_past_time_features, val_past_observed_mask, val_future_values, val_future_time_features)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the optimizer with a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 4  # Number of epochs with no improvement after which training will be stopped\n",
    "best_val_loss = float('inf')  # Set best validation loss to infinity initially\n",
    "counter = 0  # Counter to keep track of how many times the validation loss has increased\n",
    "\n",
    "# File path to save the model\n",
    "best_model_path = \"best_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7204ce-4c9b-4279-b814-0a1bdffd74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 22.2395, Validation Loss: 21.9111\n",
      "Model saved at epoch 1 with validation loss 21.9111\n",
      "Epoch [2/100], Training Loss: 21.5693, Validation Loss: 21.1813\n",
      "Model saved at epoch 2 with validation loss 21.1813\n",
      "Epoch [3/100], Training Loss: 20.8244, Validation Loss: 20.4788\n",
      "Model saved at epoch 3 with validation loss 20.4788\n",
      "Epoch [4/100], Training Loss: 20.2457, Validation Loss: 20.0373\n",
      "Model saved at epoch 4 with validation loss 20.0373\n",
      "Epoch [5/100], Training Loss: 19.9051, Validation Loss: 19.7739\n",
      "Model saved at epoch 5 with validation loss 19.7739\n",
      "Epoch [6/100], Training Loss: 19.6839, Validation Loss: 19.5867\n",
      "Model saved at epoch 6 with validation loss 19.5867\n",
      "Epoch [7/100], Training Loss: 19.5173, Validation Loss: 19.4385\n",
      "Model saved at epoch 7 with validation loss 19.4385\n",
      "Epoch [8/100], Training Loss: 19.3817, Validation Loss: 19.3138\n",
      "Model saved at epoch 8 with validation loss 19.3138\n",
      "Epoch [9/100], Training Loss: 19.2652, Validation Loss: 19.2051\n",
      "Model saved at epoch 9 with validation loss 19.2051\n",
      "Epoch [10/100], Training Loss: 19.1616, Validation Loss: 19.1083\n",
      "Model saved at epoch 10 with validation loss 19.1083\n",
      "Epoch [11/100], Training Loss: 19.0692, Validation Loss: 19.0207\n",
      "Model saved at epoch 11 with validation loss 19.0207\n",
      "Epoch [12/100], Training Loss: 18.9849, Validation Loss: 18.9404\n",
      "Model saved at epoch 12 with validation loss 18.9404\n",
      "Epoch [13/100], Training Loss: 18.9075, Validation Loss: 18.8663\n",
      "Model saved at epoch 13 with validation loss 18.8663\n",
      "Epoch [14/100], Training Loss: 18.8358, Validation Loss: 18.7970\n",
      "Model saved at epoch 14 with validation loss 18.7970\n",
      "Epoch [15/100], Training Loss: 18.7682, Validation Loss: 18.7321\n",
      "Model saved at epoch 15 with validation loss 18.7321\n",
      "Epoch [16/100], Training Loss: 18.7046, Validation Loss: 18.6707\n",
      "Model saved at epoch 16 with validation loss 18.6707\n",
      "Epoch [17/100], Training Loss: 18.6440, Validation Loss: 18.6125\n",
      "Model saved at epoch 17 with validation loss 18.6125\n",
      "Epoch [18/100], Training Loss: 18.5871, Validation Loss: 18.5571\n",
      "Model saved at epoch 18 with validation loss 18.5571\n",
      "Epoch [19/100], Training Loss: 18.5324, Validation Loss: 18.5037\n",
      "Model saved at epoch 19 with validation loss 18.5037\n",
      "Epoch [20/100], Training Loss: 18.4800, Validation Loss: 18.4526\n",
      "Model saved at epoch 20 with validation loss 18.4526\n",
      "Epoch [21/100], Training Loss: 18.4289, Validation Loss: 18.4035\n",
      "Model saved at epoch 21 with validation loss 18.4035\n",
      "Epoch [22/100], Training Loss: 18.3798, Validation Loss: 18.3559\n",
      "Model saved at epoch 22 with validation loss 18.3559\n",
      "Epoch [23/100], Training Loss: 18.3328, Validation Loss: 18.3101\n",
      "Model saved at epoch 23 with validation loss 18.3101\n",
      "Epoch [24/100], Training Loss: 18.2872, Validation Loss: 18.2656\n",
      "Model saved at epoch 24 with validation loss 18.2656\n",
      "Epoch [25/100], Training Loss: 18.2431, Validation Loss: 18.2225\n",
      "Model saved at epoch 25 with validation loss 18.2225\n",
      "Epoch [26/100], Training Loss: 18.2001, Validation Loss: 18.1807\n",
      "Model saved at epoch 26 with validation loss 18.1807\n",
      "Epoch [27/100], Training Loss: 18.1588, Validation Loss: 18.1401\n",
      "Model saved at epoch 27 with validation loss 18.1401\n",
      "Epoch [28/100], Training Loss: 18.1183, Validation Loss: 18.1007\n",
      "Model saved at epoch 28 with validation loss 18.1007\n",
      "Epoch [29/100], Training Loss: 18.0792, Validation Loss: 18.0624\n",
      "Model saved at epoch 29 with validation loss 18.0624\n",
      "Epoch [30/100], Training Loss: 18.0411, Validation Loss: 18.0249\n",
      "Model saved at epoch 30 with validation loss 18.0249\n",
      "Epoch [31/100], Training Loss: 18.0036, Validation Loss: 17.9886\n",
      "Model saved at epoch 31 with validation loss 17.9886\n",
      "Epoch [32/100], Training Loss: 17.9673, Validation Loss: 17.9533\n",
      "Model saved at epoch 32 with validation loss 17.9533\n",
      "Epoch [33/100], Training Loss: 17.9324, Validation Loss: 17.9187\n",
      "Model saved at epoch 33 with validation loss 17.9187\n",
      "Epoch [34/100], Training Loss: 17.8980, Validation Loss: 17.8850\n",
      "Model saved at epoch 34 with validation loss 17.8850\n",
      "Epoch [35/100], Training Loss: 17.8638, Validation Loss: 17.8520\n",
      "Model saved at epoch 35 with validation loss 17.8520\n",
      "Epoch [36/100], Training Loss: 17.8316, Validation Loss: 17.8195\n",
      "Model saved at epoch 36 with validation loss 17.8195\n",
      "Epoch [37/100], Training Loss: 17.7985, Validation Loss: 17.7874\n",
      "Model saved at epoch 37 with validation loss 17.7874\n",
      "Epoch [38/100], Training Loss: 17.7661, Validation Loss: 17.7556\n",
      "Model saved at epoch 38 with validation loss 17.7556\n",
      "Epoch [39/100], Training Loss: 17.7343, Validation Loss: 17.7231\n",
      "Model saved at epoch 39 with validation loss 17.7231\n",
      "Epoch [40/100], Training Loss: 17.7012, Validation Loss: 17.6899\n",
      "Model saved at epoch 40 with validation loss 17.6899\n",
      "Epoch [41/100], Training Loss: 17.6666, Validation Loss: 17.6548\n",
      "Model saved at epoch 41 with validation loss 17.6548\n",
      "Epoch [42/100], Training Loss: 17.6297, Validation Loss: 17.6146\n",
      "Model saved at epoch 42 with validation loss 17.6146\n",
      "Epoch [43/100], Training Loss: 17.5850, Validation Loss: 17.5649\n",
      "Model saved at epoch 43 with validation loss 17.5649\n",
      "Epoch [44/100], Training Loss: 17.5188, Validation Loss: 17.4715\n",
      "Model saved at epoch 44 with validation loss 17.4715\n",
      "Epoch [45/100], Training Loss: 17.1930, Validation Loss: 16.6886\n",
      "Model saved at epoch 45 with validation loss 16.6886\n",
      "Epoch [46/100], Training Loss: 16.5848, Validation Loss: 16.4858\n",
      "Model saved at epoch 46 with validation loss 16.4858\n",
      "Epoch [47/100], Training Loss: 16.4179, Validation Loss: 16.3477\n",
      "Model saved at epoch 47 with validation loss 16.3477\n",
      "Epoch [48/100], Training Loss: 16.2893, Validation Loss: 16.2225\n",
      "Model saved at epoch 48 with validation loss 16.2225\n",
      "Epoch [49/100], Training Loss: 16.1780, Validation Loss: 16.1175\n",
      "Model saved at epoch 49 with validation loss 16.1175\n",
      "Epoch [50/100], Training Loss: 16.0787, Validation Loss: 16.0239\n",
      "Model saved at epoch 50 with validation loss 16.0239\n",
      "Epoch [51/100], Training Loss: 15.9885, Validation Loss: 15.9365\n",
      "Model saved at epoch 51 with validation loss 15.9365\n",
      "Epoch [52/100], Training Loss: 15.9048, Validation Loss: 15.8539\n",
      "Model saved at epoch 52 with validation loss 15.8539\n",
      "Epoch [53/100], Training Loss: 15.8264, Validation Loss: 15.7777\n",
      "Model saved at epoch 53 with validation loss 15.7777\n",
      "Epoch [54/100], Training Loss: 15.7520, Validation Loss: 15.7066\n",
      "Model saved at epoch 54 with validation loss 15.7066\n",
      "Epoch [55/100], Training Loss: 15.6802, Validation Loss: 15.6349\n",
      "Model saved at epoch 55 with validation loss 15.6349\n",
      "Epoch [56/100], Training Loss: 15.6134, Validation Loss: 15.5688\n",
      "Model saved at epoch 56 with validation loss 15.5688\n",
      "Epoch [57/100], Training Loss: 15.5475, Validation Loss: 15.5049\n",
      "Model saved at epoch 57 with validation loss 15.5049\n",
      "Epoch [58/100], Training Loss: 15.4852, Validation Loss: 15.4436\n",
      "Model saved at epoch 58 with validation loss 15.4436\n",
      "Epoch [59/100], Training Loss: 15.4272, Validation Loss: 15.3860\n",
      "Model saved at epoch 59 with validation loss 15.3860\n",
      "Epoch [60/100], Training Loss: 15.3691, Validation Loss: 15.3331\n",
      "Model saved at epoch 60 with validation loss 15.3331\n",
      "Epoch [61/100], Training Loss: 15.3141, Validation Loss: 15.2750\n",
      "Model saved at epoch 61 with validation loss 15.2750\n",
      "Epoch [62/100], Training Loss: 15.2613, Validation Loss: 15.2231\n",
      "Model saved at epoch 62 with validation loss 15.2231\n",
      "Epoch [63/100], Training Loss: 15.2107, Validation Loss: 15.1737\n",
      "Model saved at epoch 63 with validation loss 15.1737\n",
      "Epoch [64/100], Training Loss: 15.1603, Validation Loss: 15.1253\n",
      "Model saved at epoch 64 with validation loss 15.1253\n",
      "Epoch [65/100], Training Loss: 15.1123, Validation Loss: 15.0752\n",
      "Model saved at epoch 65 with validation loss 15.0752\n",
      "Epoch [66/100], Training Loss: 15.0647, Validation Loss: 15.0290\n",
      "Model saved at epoch 66 with validation loss 15.0290\n",
      "Epoch [67/100], Training Loss: 15.0178, Validation Loss: 14.9823\n",
      "Model saved at epoch 67 with validation loss 14.9823\n",
      "Epoch [68/100], Training Loss: 14.9727, Validation Loss: 14.9385\n",
      "Model saved at epoch 68 with validation loss 14.9385\n",
      "Epoch [69/100], Training Loss: 14.9285, Validation Loss: 14.8924\n",
      "Model saved at epoch 69 with validation loss 14.8924\n",
      "Epoch [70/100], Training Loss: 14.8835, Validation Loss: 14.8504\n",
      "Model saved at epoch 70 with validation loss 14.8504\n",
      "Epoch [71/100], Training Loss: 14.8422, Validation Loss: 14.8064\n",
      "Model saved at epoch 71 with validation loss 14.8064\n",
      "Epoch [72/100], Training Loss: 14.7978, Validation Loss: 14.7716\n",
      "Model saved at epoch 72 with validation loss 14.7716\n",
      "Epoch [73/100], Training Loss: 14.7569, Validation Loss: 14.7265\n",
      "Model saved at epoch 73 with validation loss 14.7265\n",
      "Epoch [74/100], Training Loss: 14.7161, Validation Loss: 14.6871\n",
      "Model saved at epoch 74 with validation loss 14.6871\n",
      "Epoch [75/100], Training Loss: 14.6768, Validation Loss: 14.6420\n",
      "Model saved at epoch 75 with validation loss 14.6420\n",
      "Epoch [76/100], Training Loss: 14.6355, Validation Loss: 14.6118\n",
      "Model saved at epoch 76 with validation loss 14.6118\n",
      "Epoch [77/100], Training Loss: 14.5968, Validation Loss: 14.5688\n",
      "Model saved at epoch 77 with validation loss 14.5688\n",
      "Epoch [78/100], Training Loss: 14.5590, Validation Loss: 14.5261\n",
      "Model saved at epoch 78 with validation loss 14.5261\n",
      "Epoch [79/100], Training Loss: 14.5191, Validation Loss: 14.4876\n",
      "Model saved at epoch 79 with validation loss 14.4876\n",
      "Epoch [80/100], Training Loss: 14.4831, Validation Loss: 14.4524\n",
      "Model saved at epoch 80 with validation loss 14.4524\n",
      "Epoch [81/100], Training Loss: 14.4455, Validation Loss: 14.4253\n",
      "Model saved at epoch 81 with validation loss 14.4253\n",
      "Epoch [82/100], Training Loss: 14.4089, Validation Loss: 14.3775\n",
      "Model saved at epoch 82 with validation loss 14.3775\n",
      "Epoch [83/100], Training Loss: 14.3716, Validation Loss: 14.3410\n",
      "Model saved at epoch 83 with validation loss 14.3410\n",
      "Epoch [84/100], Training Loss: 14.3374, Validation Loss: 14.3103\n",
      "Model saved at epoch 84 with validation loss 14.3103\n",
      "Epoch [85/100], Training Loss: 14.3031, Validation Loss: 14.2760\n",
      "Model saved at epoch 85 with validation loss 14.2760\n",
      "Epoch [86/100], Training Loss: 14.2664, Validation Loss: 14.2361\n",
      "Model saved at epoch 86 with validation loss 14.2361\n",
      "Epoch [87/100], Training Loss: 14.2334, Validation Loss: 14.2024\n",
      "Model saved at epoch 87 with validation loss 14.2024\n",
      "Epoch [88/100], Training Loss: 14.1979, Validation Loss: 14.1681\n",
      "Model saved at epoch 88 with validation loss 14.1681\n",
      "Epoch [89/100], Training Loss: 14.1651, Validation Loss: 14.1349\n",
      "Model saved at epoch 89 with validation loss 14.1349\n",
      "Epoch [90/100], Training Loss: 14.1298, Validation Loss: 14.1099\n",
      "Model saved at epoch 90 with validation loss 14.1099\n",
      "Epoch [91/100], Training Loss: 14.0974, Validation Loss: 14.0675\n",
      "Model saved at epoch 91 with validation loss 14.0675\n",
      "Epoch [92/100], Training Loss: 14.0640, Validation Loss: 14.0449\n",
      "Model saved at epoch 92 with validation loss 14.0449\n",
      "Epoch [93/100], Training Loss: 14.0299, Validation Loss: 14.0018\n",
      "Model saved at epoch 93 with validation loss 14.0018\n",
      "Epoch [94/100], Training Loss: 13.9971, Validation Loss: 13.9689\n",
      "Model saved at epoch 94 with validation loss 13.9689\n",
      "Epoch [95/100], Training Loss: 13.9663, Validation Loss: 13.9457\n",
      "Model saved at epoch 95 with validation loss 13.9457\n",
      "Epoch [96/100], Training Loss: 13.9343, Validation Loss: 13.9053\n",
      "Model saved at epoch 96 with validation loss 13.9053\n",
      "Epoch [97/100], Training Loss: 13.8994, Validation Loss: 13.8742\n",
      "Model saved at epoch 97 with validation loss 13.8742\n",
      "Epoch [98/100], Training Loss: 13.8678, Validation Loss: 13.8409\n",
      "Model saved at epoch 98 with validation loss 13.8409\n",
      "Epoch [99/100], Training Loss: 13.8358, Validation Loss: 13.8102\n",
      "Model saved at epoch 99 with validation loss 13.8102\n",
      "Epoch [100/100], Training Loss: 13.8045, Validation Loss: 13.7786\n",
      "Model saved at epoch 100 with validation loss 13.7786\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch in train_loader:\n",
    "        batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=batch_past_values,\n",
    "            past_time_features=batch_past_time_features,\n",
    "            past_observed_mask=batch_past_observed_mask,\n",
    "            future_values=batch_future_values,\n",
    "            future_time_features=batch_future_time_features\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation loop (without backpropagation)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No gradient calculation during validation\n",
    "        for batch in val_loader:\n",
    "            batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                past_values=batch_past_values,\n",
    "                past_time_features=batch_past_time_features,\n",
    "                past_observed_mask=batch_past_observed_mask,\n",
    "                future_values=batch_future_values,\n",
    "                future_time_features=batch_future_time_features\n",
    "            )\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss  # Update best validation loss\n",
    "        counter = 0  # Reset the counter if validation loss improves\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), best_model_path)  # Save the model's state_dict to file\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        counter += 1  # Increment the counter if validation loss does not improve\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break  # Stop training if the validation loss increases for 'patience' epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1986b407-0dd1-4887-8196-b4c02871881f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))  # Load the saved best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33df6141-7088-4b47-baa6-2ea399c00a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/200], Training Loss: 13.7648, Validation Loss: 13.7374\n",
      "Model saved at epoch 101 with validation loss 13.7374\n",
      "Epoch [102/200], Training Loss: 13.7340, Validation Loss: 13.7082\n",
      "Model saved at epoch 102 with validation loss 13.7082\n",
      "Epoch [103/200], Training Loss: 13.7033, Validation Loss: 13.6767\n",
      "Model saved at epoch 103 with validation loss 13.6767\n",
      "Epoch [104/200], Training Loss: 13.6714, Validation Loss: 13.6477\n",
      "Model saved at epoch 104 with validation loss 13.6477\n",
      "Epoch [105/200], Training Loss: 13.6416, Validation Loss: 13.6199\n",
      "Model saved at epoch 105 with validation loss 13.6199\n",
      "Epoch [106/200], Training Loss: 13.6165, Validation Loss: 13.5898\n",
      "Model saved at epoch 106 with validation loss 13.5898\n",
      "Epoch [107/200], Training Loss: 13.5843, Validation Loss: 13.5601\n",
      "Model saved at epoch 107 with validation loss 13.5601\n",
      "Epoch [108/200], Training Loss: 13.5557, Validation Loss: 13.5331\n",
      "Model saved at epoch 108 with validation loss 13.5331\n",
      "Epoch [109/200], Training Loss: 13.5282, Validation Loss: 13.5082\n",
      "Model saved at epoch 109 with validation loss 13.5082\n",
      "Epoch [110/200], Training Loss: 13.4997, Validation Loss: 13.4797\n",
      "Model saved at epoch 110 with validation loss 13.4797\n",
      "Epoch [111/200], Training Loss: 13.4734, Validation Loss: 13.4506\n",
      "Model saved at epoch 111 with validation loss 13.4506\n",
      "Epoch [112/200], Training Loss: 13.4469, Validation Loss: 13.4253\n",
      "Model saved at epoch 112 with validation loss 13.4253\n",
      "Epoch [113/200], Training Loss: 13.4218, Validation Loss: 13.3994\n",
      "Model saved at epoch 113 with validation loss 13.3994\n",
      "Epoch [114/200], Training Loss: 13.3951, Validation Loss: 13.3746\n",
      "Model saved at epoch 114 with validation loss 13.3746\n",
      "Epoch [115/200], Training Loss: 13.3702, Validation Loss: 13.3531\n",
      "Model saved at epoch 115 with validation loss 13.3531\n",
      "Epoch [116/200], Training Loss: 13.3488, Validation Loss: 13.3293\n",
      "Model saved at epoch 116 with validation loss 13.3293\n",
      "Epoch [117/200], Training Loss: 13.3236, Validation Loss: 13.3268\n",
      "Model saved at epoch 117 with validation loss 13.3268\n",
      "Epoch [118/200], Training Loss: 13.3024, Validation Loss: 13.2790\n",
      "Model saved at epoch 118 with validation loss 13.2790\n",
      "Epoch [119/200], Training Loss: 13.2759, Validation Loss: 13.2564\n",
      "Model saved at epoch 119 with validation loss 13.2564\n",
      "Epoch [120/200], Training Loss: 13.2523, Validation Loss: 13.2358\n",
      "Model saved at epoch 120 with validation loss 13.2358\n",
      "Epoch [121/200], Training Loss: 13.2325, Validation Loss: 13.2119\n",
      "Model saved at epoch 121 with validation loss 13.2119\n",
      "Epoch [122/200], Training Loss: 13.2079, Validation Loss: 13.1905\n",
      "Model saved at epoch 122 with validation loss 13.1905\n",
      "Epoch [123/200], Training Loss: 13.1880, Validation Loss: 13.1677\n",
      "Model saved at epoch 123 with validation loss 13.1677\n",
      "Epoch [124/200], Training Loss: 13.1652, Validation Loss: 13.1502\n",
      "Model saved at epoch 124 with validation loss 13.1502\n",
      "Epoch [125/200], Training Loss: 13.1473, Validation Loss: 13.1293\n",
      "Model saved at epoch 125 with validation loss 13.1293\n",
      "Epoch [126/200], Training Loss: 13.1243, Validation Loss: 13.1059\n",
      "Model saved at epoch 126 with validation loss 13.1059\n",
      "Epoch [127/200], Training Loss: 13.1049, Validation Loss: 13.0837\n",
      "Model saved at epoch 127 with validation loss 13.0837\n",
      "Epoch [128/200], Training Loss: 13.0813, Validation Loss: 13.0646\n",
      "Model saved at epoch 128 with validation loss 13.0646\n",
      "Epoch [129/200], Training Loss: 13.0618, Validation Loss: 13.0522\n",
      "Model saved at epoch 129 with validation loss 13.0522\n",
      "Epoch [130/200], Training Loss: 13.0439, Validation Loss: 13.0348\n",
      "Model saved at epoch 130 with validation loss 13.0348\n",
      "Epoch [131/200], Training Loss: 13.0225, Validation Loss: 13.0058\n",
      "Model saved at epoch 131 with validation loss 13.0058\n",
      "Epoch [132/200], Training Loss: 13.0020, Validation Loss: 12.9855\n",
      "Model saved at epoch 132 with validation loss 12.9855\n",
      "Epoch [133/200], Training Loss: 12.9840, Validation Loss: 12.9674\n",
      "Model saved at epoch 133 with validation loss 12.9674\n",
      "Epoch [134/200], Training Loss: 12.9666, Validation Loss: 12.9514\n",
      "Model saved at epoch 134 with validation loss 12.9514\n",
      "Epoch [135/200], Training Loss: 12.9454, Validation Loss: 12.9293\n",
      "Model saved at epoch 135 with validation loss 12.9293\n",
      "Epoch [136/200], Training Loss: 12.9292, Validation Loss: 12.9170\n",
      "Model saved at epoch 136 with validation loss 12.9170\n",
      "Epoch [137/200], Training Loss: 12.9090, Validation Loss: 12.8931\n",
      "Model saved at epoch 137 with validation loss 12.8931\n",
      "Epoch [138/200], Training Loss: 12.8918, Validation Loss: 12.8794\n",
      "Model saved at epoch 138 with validation loss 12.8794\n",
      "Epoch [139/200], Training Loss: 12.8760, Validation Loss: 12.8547\n",
      "Model saved at epoch 139 with validation loss 12.8547\n",
      "Epoch [140/200], Training Loss: 12.8566, Validation Loss: 12.8373\n",
      "Model saved at epoch 140 with validation loss 12.8373\n",
      "Epoch [141/200], Training Loss: 12.8375, Validation Loss: 12.8208\n",
      "Model saved at epoch 141 with validation loss 12.8208\n",
      "Epoch [142/200], Training Loss: 12.8194, Validation Loss: 12.8062\n",
      "Model saved at epoch 142 with validation loss 12.8062\n",
      "Epoch [143/200], Training Loss: 12.8049, Validation Loss: 12.8244\n",
      "Epoch [144/200], Training Loss: 12.7860, Validation Loss: 12.7688\n",
      "Model saved at epoch 144 with validation loss 12.7688\n",
      "Epoch [145/200], Training Loss: 12.7707, Validation Loss: 12.7527\n",
      "Model saved at epoch 145 with validation loss 12.7527\n",
      "Epoch [146/200], Training Loss: 12.7527, Validation Loss: 12.7396\n",
      "Model saved at epoch 146 with validation loss 12.7396\n",
      "Epoch [147/200], Training Loss: 12.7363, Validation Loss: 12.7247\n",
      "Model saved at epoch 147 with validation loss 12.7247\n",
      "Epoch [148/200], Training Loss: 12.7264, Validation Loss: 12.7038\n",
      "Model saved at epoch 148 with validation loss 12.7038\n",
      "Epoch [149/200], Training Loss: 12.7033, Validation Loss: 12.6913\n",
      "Model saved at epoch 149 with validation loss 12.6913\n",
      "Epoch [150/200], Training Loss: 12.6874, Validation Loss: 12.6755\n",
      "Model saved at epoch 150 with validation loss 12.6755\n",
      "Epoch [151/200], Training Loss: 12.6732, Validation Loss: 12.6557\n",
      "Model saved at epoch 151 with validation loss 12.6557\n",
      "Epoch [152/200], Training Loss: 12.6588, Validation Loss: 12.6488\n",
      "Model saved at epoch 152 with validation loss 12.6488\n",
      "Epoch [153/200], Training Loss: 12.6430, Validation Loss: 12.6291\n",
      "Model saved at epoch 153 with validation loss 12.6291\n",
      "Epoch [154/200], Training Loss: 12.6273, Validation Loss: 12.6092\n",
      "Model saved at epoch 154 with validation loss 12.6092\n",
      "Epoch [155/200], Training Loss: 12.6130, Validation Loss: 12.6019\n",
      "Model saved at epoch 155 with validation loss 12.6019\n",
      "Epoch [156/200], Training Loss: 12.5992, Validation Loss: 12.5865\n",
      "Model saved at epoch 156 with validation loss 12.5865\n",
      "Epoch [157/200], Training Loss: 12.5835, Validation Loss: 12.5657\n",
      "Model saved at epoch 157 with validation loss 12.5657\n",
      "Epoch [158/200], Training Loss: 12.5672, Validation Loss: 12.5534\n",
      "Model saved at epoch 158 with validation loss 12.5534\n",
      "Epoch [159/200], Training Loss: 12.5544, Validation Loss: 12.5503\n",
      "Model saved at epoch 159 with validation loss 12.5503\n",
      "Epoch [160/200], Training Loss: 12.5391, Validation Loss: 12.5365\n",
      "Model saved at epoch 160 with validation loss 12.5365\n",
      "Epoch [161/200], Training Loss: 12.5240, Validation Loss: 12.5090\n",
      "Model saved at epoch 161 with validation loss 12.5090\n",
      "Epoch [162/200], Training Loss: 12.5148, Validation Loss: 12.5132\n",
      "Epoch [163/200], Training Loss: 12.4979, Validation Loss: 12.4887\n",
      "Model saved at epoch 163 with validation loss 12.4887\n",
      "Epoch [164/200], Training Loss: 12.4860, Validation Loss: 12.4721\n",
      "Model saved at epoch 164 with validation loss 12.4721\n",
      "Epoch [165/200], Training Loss: 12.4737, Validation Loss: 12.4593\n",
      "Model saved at epoch 165 with validation loss 12.4593\n",
      "Epoch [166/200], Training Loss: 12.4580, Validation Loss: 12.4618\n",
      "Epoch [167/200], Training Loss: 12.4459, Validation Loss: 12.4285\n",
      "Model saved at epoch 167 with validation loss 12.4285\n",
      "Epoch [168/200], Training Loss: 12.4333, Validation Loss: 12.4268\n",
      "Model saved at epoch 168 with validation loss 12.4268\n",
      "Epoch [169/200], Training Loss: 12.4203, Validation Loss: 12.4007\n",
      "Model saved at epoch 169 with validation loss 12.4007\n",
      "Epoch [170/200], Training Loss: 12.4081, Validation Loss: 12.3992\n",
      "Model saved at epoch 170 with validation loss 12.3992\n",
      "Epoch [171/200], Training Loss: 12.3929, Validation Loss: 12.3748\n",
      "Model saved at epoch 171 with validation loss 12.3748\n",
      "Epoch [172/200], Training Loss: 12.3826, Validation Loss: 12.3616\n",
      "Model saved at epoch 172 with validation loss 12.3616\n",
      "Epoch [173/200], Training Loss: 12.3711, Validation Loss: 12.3605\n",
      "Model saved at epoch 173 with validation loss 12.3605\n",
      "Epoch [174/200], Training Loss: 12.3574, Validation Loss: 12.3370\n",
      "Model saved at epoch 174 with validation loss 12.3370\n",
      "Epoch [175/200], Training Loss: 12.3448, Validation Loss: 12.3333\n",
      "Model saved at epoch 175 with validation loss 12.3333\n",
      "Epoch [176/200], Training Loss: 12.3310, Validation Loss: 12.3159\n",
      "Model saved at epoch 176 with validation loss 12.3159\n",
      "Epoch [177/200], Training Loss: 12.3201, Validation Loss: 12.3015\n",
      "Model saved at epoch 177 with validation loss 12.3015\n",
      "Epoch [178/200], Training Loss: 12.3073, Validation Loss: 12.2913\n",
      "Model saved at epoch 178 with validation loss 12.2913\n",
      "Epoch [179/200], Training Loss: 12.2954, Validation Loss: 12.2764\n",
      "Model saved at epoch 179 with validation loss 12.2764\n",
      "Epoch [180/200], Training Loss: 12.2830, Validation Loss: 12.2660\n",
      "Model saved at epoch 180 with validation loss 12.2660\n",
      "Epoch [181/200], Training Loss: 12.2760, Validation Loss: 12.2571\n",
      "Model saved at epoch 181 with validation loss 12.2571\n",
      "Epoch [182/200], Training Loss: 12.2666, Validation Loss: 12.2436\n",
      "Model saved at epoch 182 with validation loss 12.2436\n",
      "Epoch [183/200], Training Loss: 12.2483, Validation Loss: 12.2317\n",
      "Model saved at epoch 183 with validation loss 12.2317\n",
      "Epoch [184/200], Training Loss: 12.2414, Validation Loss: 12.2340\n",
      "Epoch [185/200], Training Loss: 12.2271, Validation Loss: 12.2104\n",
      "Model saved at epoch 185 with validation loss 12.2104\n",
      "Epoch [186/200], Training Loss: 12.2194, Validation Loss: 12.2263\n",
      "Epoch [187/200], Training Loss: 12.2109, Validation Loss: 12.1869\n",
      "Model saved at epoch 187 with validation loss 12.1869\n",
      "Epoch [188/200], Training Loss: 12.2023, Validation Loss: 12.1804\n",
      "Model saved at epoch 188 with validation loss 12.1804\n",
      "Epoch [189/200], Training Loss: 12.1821, Validation Loss: 12.1654\n",
      "Model saved at epoch 189 with validation loss 12.1654\n",
      "Epoch [190/200], Training Loss: 12.1710, Validation Loss: 12.1671\n",
      "Epoch [191/200], Training Loss: 12.1627, Validation Loss: 12.1553\n",
      "Model saved at epoch 191 with validation loss 12.1553\n",
      "Epoch [192/200], Training Loss: 12.1561, Validation Loss: 12.1388\n",
      "Model saved at epoch 192 with validation loss 12.1388\n",
      "Epoch [193/200], Training Loss: 12.1410, Validation Loss: 12.1419\n",
      "Epoch [194/200], Training Loss: 12.1362, Validation Loss: 12.1385\n",
      "Model saved at epoch 194 with validation loss 12.1385\n",
      "Epoch [195/200], Training Loss: 12.1241, Validation Loss: 12.1533\n",
      "Epoch [196/200], Training Loss: 12.1156, Validation Loss: 12.0935\n",
      "Model saved at epoch 196 with validation loss 12.0935\n",
      "Epoch [197/200], Training Loss: 12.1048, Validation Loss: 12.0825\n",
      "Model saved at epoch 197 with validation loss 12.0825\n",
      "Epoch [198/200], Training Loss: 12.0993, Validation Loss: 12.1010\n",
      "Epoch [199/200], Training Loss: 12.0830, Validation Loss: 12.0859\n",
      "Epoch [200/200], Training Loss: 12.0779, Validation Loss: 12.0809\n",
      "Model saved at epoch 200 with validation loss 12.0809\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200  # Set total number of epochs to 200\n",
    "\n",
    "for epoch in range(100, num_epochs):  # Start from epoch 100 to 200\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training loop (as before)\n",
    "    for batch in train_loader:\n",
    "        batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=batch_past_values,\n",
    "            past_time_features=batch_past_time_features,\n",
    "            past_observed_mask=batch_past_observed_mask,\n",
    "            future_values=batch_future_values,\n",
    "            future_time_features=batch_future_time_features\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                past_values=batch_past_values,\n",
    "                past_time_features=batch_past_time_features,\n",
    "                past_observed_mask=batch_past_observed_mask,\n",
    "                future_values=batch_future_values,\n",
    "                future_time_features=batch_future_time_features\n",
    "            )\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8061665b-356c-48f1-a33b-58b720fc1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/400], Training Loss: 12.0642, Validation Loss: 12.0632\n",
      "Model saved at epoch 201 with validation loss 12.0632\n",
      "Epoch [202/400], Training Loss: 12.0601, Validation Loss: 12.0337\n",
      "Model saved at epoch 202 with validation loss 12.0337\n",
      "Epoch [203/400], Training Loss: 12.0428, Validation Loss: 12.0337\n",
      "Model saved at epoch 203 with validation loss 12.0337\n",
      "Epoch [204/400], Training Loss: 12.0363, Validation Loss: 12.0202\n",
      "Model saved at epoch 204 with validation loss 12.0202\n",
      "Epoch [205/400], Training Loss: 12.0288, Validation Loss: 12.0275\n",
      "Epoch [206/400], Training Loss: 12.0174, Validation Loss: 11.9967\n",
      "Model saved at epoch 206 with validation loss 11.9967\n",
      "Epoch [207/400], Training Loss: 12.0056, Validation Loss: 11.9903\n",
      "Model saved at epoch 207 with validation loss 11.9903\n",
      "Epoch [208/400], Training Loss: 12.0023, Validation Loss: 11.9777\n",
      "Model saved at epoch 208 with validation loss 11.9777\n",
      "Epoch [209/400], Training Loss: 11.9906, Validation Loss: 11.9697\n",
      "Model saved at epoch 209 with validation loss 11.9697\n",
      "Epoch [210/400], Training Loss: 11.9811, Validation Loss: 12.0021\n",
      "Epoch [211/400], Training Loss: 11.9729, Validation Loss: 11.9982\n",
      "Epoch [212/400], Training Loss: 11.9698, Validation Loss: 11.9484\n",
      "Model saved at epoch 212 with validation loss 11.9484\n",
      "Epoch [213/400], Training Loss: 11.9565, Validation Loss: 11.9344\n",
      "Model saved at epoch 213 with validation loss 11.9344\n",
      "Epoch [214/400], Training Loss: 11.9464, Validation Loss: 11.9498\n",
      "Epoch [215/400], Training Loss: 11.9418, Validation Loss: 11.9495\n",
      "Epoch [216/400], Training Loss: 11.9299, Validation Loss: 11.9682\n",
      "Epoch [217/400], Training Loss: 11.9253, Validation Loss: 11.9170\n",
      "Model saved at epoch 217 with validation loss 11.9170\n",
      "Epoch [218/400], Training Loss: 11.9107, Validation Loss: 11.8917\n",
      "Model saved at epoch 218 with validation loss 11.8917\n",
      "Epoch [219/400], Training Loss: 11.9053, Validation Loss: 11.8867\n",
      "Model saved at epoch 219 with validation loss 11.8867\n",
      "Epoch [220/400], Training Loss: 11.8982, Validation Loss: 11.8760\n",
      "Model saved at epoch 220 with validation loss 11.8760\n",
      "Epoch [221/400], Training Loss: 11.8920, Validation Loss: 11.8700\n",
      "Model saved at epoch 221 with validation loss 11.8700\n",
      "Epoch [222/400], Training Loss: 11.8830, Validation Loss: 11.8749\n",
      "Epoch [223/400], Training Loss: 11.8755, Validation Loss: 11.8520\n",
      "Model saved at epoch 223 with validation loss 11.8520\n",
      "Epoch [224/400], Training Loss: 11.8663, Validation Loss: 11.8558\n",
      "Epoch [225/400], Training Loss: 11.8538, Validation Loss: 11.8404\n",
      "Model saved at epoch 225 with validation loss 11.8404\n",
      "Epoch [226/400], Training Loss: 11.8501, Validation Loss: 11.8987\n",
      "Epoch [227/400], Training Loss: 11.8477, Validation Loss: 11.8223\n",
      "Model saved at epoch 227 with validation loss 11.8223\n",
      "Epoch [228/400], Training Loss: 11.8339, Validation Loss: 11.8675\n",
      "Epoch [229/400], Training Loss: 11.8280, Validation Loss: 11.8062\n",
      "Model saved at epoch 229 with validation loss 11.8062\n",
      "Epoch [230/400], Training Loss: 11.8253, Validation Loss: 11.8415\n",
      "Epoch [231/400], Training Loss: 11.8109, Validation Loss: 11.8037\n",
      "Model saved at epoch 231 with validation loss 11.8037\n",
      "Epoch [232/400], Training Loss: 11.8102, Validation Loss: 11.8199\n",
      "Epoch [233/400], Training Loss: 11.7978, Validation Loss: 11.7823\n",
      "Model saved at epoch 233 with validation loss 11.7823\n",
      "Epoch [234/400], Training Loss: 11.7884, Validation Loss: 11.7821\n",
      "Model saved at epoch 234 with validation loss 11.7821\n",
      "Epoch [235/400], Training Loss: 11.7844, Validation Loss: 11.7679\n",
      "Model saved at epoch 235 with validation loss 11.7679\n",
      "Epoch [236/400], Training Loss: 11.7749, Validation Loss: 11.7540\n",
      "Model saved at epoch 236 with validation loss 11.7540\n",
      "Epoch [237/400], Training Loss: 11.7789, Validation Loss: 11.7456\n",
      "Model saved at epoch 237 with validation loss 11.7456\n",
      "Epoch [238/400], Training Loss: 11.7650, Validation Loss: 11.7408\n",
      "Model saved at epoch 238 with validation loss 11.7408\n",
      "Epoch [239/400], Training Loss: 11.7528, Validation Loss: 11.7339\n",
      "Model saved at epoch 239 with validation loss 11.7339\n",
      "Epoch [240/400], Training Loss: 11.7448, Validation Loss: 11.7271\n",
      "Model saved at epoch 240 with validation loss 11.7271\n",
      "Epoch [241/400], Training Loss: 11.7401, Validation Loss: 11.7201\n",
      "Model saved at epoch 241 with validation loss 11.7201\n",
      "Epoch [242/400], Training Loss: 11.7361, Validation Loss: 11.7324\n",
      "Epoch [243/400], Training Loss: 11.7298, Validation Loss: 11.7215\n",
      "Epoch [244/400], Training Loss: 11.7236, Validation Loss: 11.7033\n",
      "Model saved at epoch 244 with validation loss 11.7033\n",
      "Epoch [245/400], Training Loss: 11.7208, Validation Loss: 11.6946\n",
      "Model saved at epoch 245 with validation loss 11.6946\n",
      "Epoch [246/400], Training Loss: 11.7102, Validation Loss: 11.6888\n",
      "Model saved at epoch 246 with validation loss 11.6888\n",
      "Epoch [247/400], Training Loss: 11.6978, Validation Loss: 11.6780\n",
      "Model saved at epoch 247 with validation loss 11.6780\n",
      "Epoch [248/400], Training Loss: 11.6924, Validation Loss: 11.6719\n",
      "Model saved at epoch 248 with validation loss 11.6719\n",
      "Epoch [249/400], Training Loss: 11.6906, Validation Loss: 11.6642\n",
      "Model saved at epoch 249 with validation loss 11.6642\n",
      "Epoch [250/400], Training Loss: 11.6873, Validation Loss: 11.6573\n",
      "Model saved at epoch 250 with validation loss 11.6573\n",
      "Epoch [251/400], Training Loss: 11.6755, Validation Loss: 11.6518\n",
      "Model saved at epoch 251 with validation loss 11.6518\n",
      "Epoch [252/400], Training Loss: 11.6734, Validation Loss: 11.6592\n",
      "Epoch [253/400], Training Loss: 11.6635, Validation Loss: 11.6525\n",
      "Epoch [254/400], Training Loss: 11.6607, Validation Loss: 11.6476\n",
      "Model saved at epoch 254 with validation loss 11.6476\n",
      "Epoch [255/400], Training Loss: 11.6555, Validation Loss: 11.6319\n",
      "Model saved at epoch 255 with validation loss 11.6319\n",
      "Epoch [256/400], Training Loss: 11.6537, Validation Loss: 11.6290\n",
      "Model saved at epoch 256 with validation loss 11.6290\n",
      "Epoch [257/400], Training Loss: 11.6371, Validation Loss: 11.6508\n",
      "Epoch [258/400], Training Loss: 11.6321, Validation Loss: 11.6233\n",
      "Model saved at epoch 258 with validation loss 11.6233\n",
      "Epoch [259/400], Training Loss: 11.6311, Validation Loss: 11.6039\n",
      "Model saved at epoch 259 with validation loss 11.6039\n",
      "Epoch [260/400], Training Loss: 11.6199, Validation Loss: 11.5970\n",
      "Model saved at epoch 260 with validation loss 11.5970\n",
      "Epoch [261/400], Training Loss: 11.6254, Validation Loss: 11.5910\n",
      "Model saved at epoch 261 with validation loss 11.5910\n",
      "Epoch [262/400], Training Loss: 11.6120, Validation Loss: 11.5964\n",
      "Epoch [263/400], Training Loss: 11.6037, Validation Loss: 11.5839\n",
      "Model saved at epoch 263 with validation loss 11.5839\n",
      "Epoch [264/400], Training Loss: 11.5969, Validation Loss: 11.5800\n",
      "Model saved at epoch 264 with validation loss 11.5800\n",
      "Epoch [265/400], Training Loss: 11.5979, Validation Loss: 11.5673\n",
      "Model saved at epoch 265 with validation loss 11.5673\n",
      "Epoch [266/400], Training Loss: 11.5972, Validation Loss: 11.5682\n",
      "Epoch [267/400], Training Loss: 11.5834, Validation Loss: 11.5652\n",
      "Model saved at epoch 267 with validation loss 11.5652\n",
      "Epoch [268/400], Training Loss: 11.5771, Validation Loss: 11.5800\n",
      "Epoch [269/400], Training Loss: 11.5726, Validation Loss: 11.5513\n",
      "Model saved at epoch 269 with validation loss 11.5513\n",
      "Epoch [270/400], Training Loss: 11.5782, Validation Loss: 11.5427\n",
      "Model saved at epoch 270 with validation loss 11.5427\n",
      "Epoch [271/400], Training Loss: 11.5671, Validation Loss: 11.5417\n",
      "Model saved at epoch 271 with validation loss 11.5417\n",
      "Epoch [272/400], Training Loss: 11.5513, Validation Loss: 11.5323\n",
      "Model saved at epoch 272 with validation loss 11.5323\n",
      "Epoch [273/400], Training Loss: 11.5573, Validation Loss: 11.5279\n",
      "Model saved at epoch 273 with validation loss 11.5279\n",
      "Epoch [274/400], Training Loss: 11.5446, Validation Loss: 11.5237\n",
      "Model saved at epoch 274 with validation loss 11.5237\n",
      "Epoch [275/400], Training Loss: 11.5361, Validation Loss: 11.5220\n",
      "Model saved at epoch 275 with validation loss 11.5220\n",
      "Epoch [276/400], Training Loss: 11.5429, Validation Loss: 11.5591\n",
      "Epoch [277/400], Training Loss: 11.5311, Validation Loss: 11.5304\n",
      "Epoch [278/400], Training Loss: 11.5307, Validation Loss: 11.5113\n",
      "Model saved at epoch 278 with validation loss 11.5113\n",
      "Epoch [279/400], Training Loss: 11.5210, Validation Loss: 11.5170\n",
      "Epoch [280/400], Training Loss: 11.5191, Validation Loss: 11.4985\n",
      "Model saved at epoch 280 with validation loss 11.4985\n",
      "Epoch [281/400], Training Loss: 11.5110, Validation Loss: 11.4845\n",
      "Model saved at epoch 281 with validation loss 11.4845\n",
      "Epoch [282/400], Training Loss: 11.5175, Validation Loss: 11.4832\n",
      "Model saved at epoch 282 with validation loss 11.4832\n",
      "Epoch [283/400], Training Loss: 11.5052, Validation Loss: 11.4740\n",
      "Model saved at epoch 283 with validation loss 11.4740\n",
      "Epoch [284/400], Training Loss: 11.4988, Validation Loss: 11.4780\n",
      "Epoch [285/400], Training Loss: 11.4910, Validation Loss: 11.5385\n",
      "Epoch [286/400], Training Loss: 11.4900, Validation Loss: 11.4599\n",
      "Model saved at epoch 286 with validation loss 11.4599\n",
      "Epoch [287/400], Training Loss: 11.4819, Validation Loss: 11.4875\n",
      "Epoch [288/400], Training Loss: 11.4828, Validation Loss: 11.4519\n",
      "Model saved at epoch 288 with validation loss 11.4519\n",
      "Epoch [289/400], Training Loss: 11.4786, Validation Loss: 11.4804\n",
      "Epoch [290/400], Training Loss: 11.4726, Validation Loss: 11.4780\n",
      "Epoch [291/400], Training Loss: 11.4634, Validation Loss: 11.4408\n",
      "Model saved at epoch 291 with validation loss 11.4408\n",
      "Epoch [292/400], Training Loss: 11.4575, Validation Loss: 11.4418\n",
      "Epoch [293/400], Training Loss: 11.4599, Validation Loss: 11.4744\n",
      "Epoch [294/400], Training Loss: 11.4598, Validation Loss: 11.4968\n",
      "Epoch [295/400], Training Loss: 11.4442, Validation Loss: 11.4529\n",
      "Early stopping at epoch 295 due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 400  # Set total number of epochs to 200\n",
    "\n",
    "for epoch in range(200, num_epochs):  # Start from epoch 100 to 200\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training loop (as before)\n",
    "    for batch in train_loader:\n",
    "        batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=batch_past_values,\n",
    "            past_time_features=batch_past_time_features,\n",
    "            past_observed_mask=batch_past_observed_mask,\n",
    "            future_values=batch_future_values,\n",
    "            future_time_features=batch_future_time_features\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                past_values=batch_past_values,\n",
    "                past_time_features=batch_past_time_features,\n",
    "                past_observed_mask=batch_past_observed_mask,\n",
    "                future_values=batch_future_values,\n",
    "                future_time_features=batch_future_time_features\n",
    "            )\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a93457-0b9e-4412-acff-b8ee506bb81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [401/2000], Training Loss: 11.4466, Validation Loss: 11.4165\n",
      "Model saved at epoch 401 with validation loss 11.4165\n",
      "Epoch [402/2000], Training Loss: 11.4465, Validation Loss: 11.4120\n",
      "Model saved at epoch 402 with validation loss 11.4120\n",
      "Epoch [403/2000], Training Loss: 11.4356, Validation Loss: 11.4574\n",
      "Epoch [404/2000], Training Loss: 11.4376, Validation Loss: 11.4184\n",
      "Epoch [405/2000], Training Loss: 11.4258, Validation Loss: 11.4711\n",
      "Epoch [406/2000], Training Loss: 11.4212, Validation Loss: 11.3940\n",
      "Model saved at epoch 406 with validation loss 11.3940\n",
      "Epoch [407/2000], Training Loss: 11.4155, Validation Loss: 11.4345\n",
      "Epoch [408/2000], Training Loss: 11.4236, Validation Loss: 11.4039\n",
      "Epoch [409/2000], Training Loss: 11.4142, Validation Loss: 11.3929\n",
      "Model saved at epoch 409 with validation loss 11.3929\n",
      "Epoch [410/2000], Training Loss: 11.4076, Validation Loss: 11.4598\n",
      "Epoch [411/2000], Training Loss: 11.4035, Validation Loss: 11.3833\n",
      "Model saved at epoch 411 with validation loss 11.3833\n",
      "Epoch [412/2000], Training Loss: 11.4033, Validation Loss: 11.4059\n",
      "Epoch [413/2000], Training Loss: 11.3969, Validation Loss: 11.3694\n",
      "Model saved at epoch 413 with validation loss 11.3694\n",
      "Epoch [414/2000], Training Loss: 11.3871, Validation Loss: 11.3608\n",
      "Model saved at epoch 414 with validation loss 11.3608\n",
      "Epoch [415/2000], Training Loss: 11.3908, Validation Loss: 11.4209\n",
      "Epoch [416/2000], Training Loss: 11.3887, Validation Loss: 11.3539\n",
      "Model saved at epoch 416 with validation loss 11.3539\n",
      "Epoch [417/2000], Training Loss: 11.3821, Validation Loss: 11.3517\n",
      "Model saved at epoch 417 with validation loss 11.3517\n",
      "Epoch [418/2000], Training Loss: 11.3852, Validation Loss: 11.3479\n",
      "Model saved at epoch 418 with validation loss 11.3479\n",
      "Epoch [419/2000], Training Loss: 11.3698, Validation Loss: 11.3432\n",
      "Model saved at epoch 419 with validation loss 11.3432\n",
      "Epoch [420/2000], Training Loss: 11.3720, Validation Loss: 11.3390\n",
      "Model saved at epoch 420 with validation loss 11.3390\n",
      "Epoch [421/2000], Training Loss: 11.3590, Validation Loss: 11.3663\n",
      "Epoch [422/2000], Training Loss: 11.3636, Validation Loss: 11.3485\n",
      "Epoch [423/2000], Training Loss: 11.3603, Validation Loss: 11.3261\n",
      "Model saved at epoch 423 with validation loss 11.3261\n",
      "Epoch [424/2000], Training Loss: 11.3544, Validation Loss: 11.3305\n",
      "Epoch [425/2000], Training Loss: 11.3494, Validation Loss: 11.3332\n",
      "Epoch [426/2000], Training Loss: 11.3536, Validation Loss: 11.3132\n",
      "Model saved at epoch 426 with validation loss 11.3132\n",
      "Epoch [427/2000], Training Loss: 11.3536, Validation Loss: 11.3098\n",
      "Model saved at epoch 427 with validation loss 11.3098\n",
      "Epoch [428/2000], Training Loss: 11.3332, Validation Loss: 11.3743\n",
      "Epoch [429/2000], Training Loss: 11.3404, Validation Loss: 11.3201\n",
      "Epoch [430/2000], Training Loss: 11.3361, Validation Loss: 11.3221\n",
      "Epoch [431/2000], Training Loss: 11.3253, Validation Loss: 11.3052\n",
      "Model saved at epoch 431 with validation loss 11.3052\n",
      "Epoch [432/2000], Training Loss: 11.3266, Validation Loss: 11.3225\n",
      "Epoch [433/2000], Training Loss: 11.3280, Validation Loss: 11.2943\n",
      "Model saved at epoch 433 with validation loss 11.2943\n",
      "Epoch [434/2000], Training Loss: 11.3196, Validation Loss: 11.3212\n",
      "Epoch [435/2000], Training Loss: 11.3132, Validation Loss: 11.2983\n",
      "Epoch [436/2000], Training Loss: 11.3214, Validation Loss: 11.2962\n",
      "Epoch [437/2000], Training Loss: 11.3116, Validation Loss: 11.2795\n",
      "Model saved at epoch 437 with validation loss 11.2795\n",
      "Epoch [438/2000], Training Loss: 11.3127, Validation Loss: 11.3225\n",
      "Epoch [439/2000], Training Loss: 11.3047, Validation Loss: 11.3227\n",
      "Epoch [440/2000], Training Loss: 11.2966, Validation Loss: 11.2941\n",
      "Epoch [441/2000], Training Loss: 11.2957, Validation Loss: 11.2980\n",
      "Early stopping at epoch 441 due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000  # Set total number of epochs to 2000\n",
    "\n",
    "for epoch in range(400, num_epochs):  # Start from epoch 100 to 200\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Training loop (as before)\n",
    "    for batch in train_loader:\n",
    "        batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            past_values=batch_past_values,\n",
    "            past_time_features=batch_past_time_features,\n",
    "            past_observed_mask=batch_past_observed_mask,\n",
    "            future_values=batch_future_values,\n",
    "            future_time_features=batch_future_time_features\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Validation loop\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_past_values, batch_past_time_features, batch_past_observed_mask, batch_future_values, batch_future_time_features = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                past_values=batch_past_values,\n",
    "                past_time_features=batch_past_time_features,\n",
    "                past_observed_mask=batch_past_observed_mask,\n",
    "                future_values=batch_future_values,\n",
    "                future_time_features=batch_future_time_features\n",
    "            )\n",
    "            \n",
    "            # Compute validation loss\n",
    "            val_loss += outputs.loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print training and validation losses for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check and save the model if validation loss improves\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Model saved at epoch {epoch+1} with validation loss {avg_val_loss:.4f}\")\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8562a-188d-4806-9166-3743e1e95e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
